{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDF Files Splitting\n",
    "\n",
    "By: Adam Li\n",
    "\n",
    "3/26/19: In order to allow facilitation of preprocessing of edf files into fif+json pairs, certain edf files need to be either split, or they need to be combined into one file in order to facilitate further downstream analysis.\n",
    "\n",
    "E.g.\n",
    "- EDF file with multiple seizures -> {edf_file01, edf_file02, ...}\n",
    "- one seizure split into multiple edf files -> {edf_file}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyedflib # pip install pyedflib\n",
    "from datetime import datetime\n",
    "import mne\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "import eztrack\n",
    "from eztrack.edp.format.formatter_raw import ConvertEDFiEEG\n",
    "from eztrack.edp.loaders.dataset.timeseries.ieegrecording import iEEGRecording\n",
    "from eztrack.edp.utils.utils import writejsonfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import timeit\n",
    "# Import magic commands for jupyter notebook \n",
    "# - autoreloading a module\n",
    "# - profiling functions for memory usage and scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitEDF():\n",
    "    def __init__(self, edffile, dataset_ids):\n",
    "        self.file = edffile\n",
    "        self.dataset_ids = dataset_ids\n",
    "            \n",
    "    def load_file(self):\n",
    "        # initialize converter\n",
    "        edfconverter = ConvertEDFiEEG(datatype='ieeg')\n",
    "        # load in the dataset and create metadata object\n",
    "        edfconverter.load_file(filepath=self.file)\n",
    "\n",
    "        # load in info data structure and edf annotated events\n",
    "        edfconverter.extract_info_and_events(pat_id=patid, autofind_markers=False)\n",
    "\n",
    "        return edfconverter\n",
    "    \n",
    "    def split_datasets(self):\n",
    "        # go through each datasetid string and find it in events\n",
    "        for dataset_id in self.dataset_ids:\n",
    "            pass\n",
    "    def _update_dict(self, master_dict, appendage_dict):\n",
    "        TIME_DEPENDENT_KEYS = ['length_of_recording',\n",
    "                               'events', \n",
    "                               'onset', \n",
    "                               'termination']\n",
    "\n",
    "        prevlen = master_dict['length_of_recording']\n",
    "        # samplerate = master_dict['samplerate']\n",
    "        samplerate = self.samplerate\n",
    "        prevsec = self._convert_sec(prevlen, samplerate)\n",
    "\n",
    "        # print(\"Lengths of recordings: \", prevlen, samplerate, prevsec)\n",
    "        for key in appendage_dict.keys():\n",
    "            if key in TIME_DEPENDENT_KEYS:\n",
    "                if key == 'length_of_recording':\n",
    "                    master_dict[key] = appendage_dict[key] + prevlen\n",
    "                elif key == 'onset' or key == 'termination':\n",
    "                    master_dict[key] = appendage_dict[key] + prevsec\n",
    "                elif key == 'events':\n",
    "                    master_dict[key] = self._concat_events(master_dict[key],\n",
    "                                                           appendage_dict[key],\n",
    "                                                           prevsec)\n",
    "            if key not in master_dict.keys():\n",
    "                master_dict[key] = appendage_dict[key]\n",
    "\n",
    "        return master_dict\n",
    "\n",
    "    def _convert_sec(self, index, samplerate):\n",
    "        return np.divide(index, samplerate)\n",
    "\n",
    "    def _concat_events(self, events_list, new_events, recording_length_seconds):\n",
    "        for event in new_events:\n",
    "            new_event = event\n",
    "            new_event[0] = float(new_event[0]) + recording_length_seconds\n",
    "            events_list = np.concatenate(\n",
    "                (events_list, np.expand_dims(new_event, axis=0)), axis=0)\n",
    "            \n",
    "        return events_list\n",
    "    \n",
    "    def save_fif(self, fif_raw, dataset_metadata, datafilepath, replace=False):\n",
    "        \"\"\"\n",
    "        Conversion function for the rawdata + metadata into a .fif file format with accompanying metadata .json\n",
    "        object.\n",
    "\n",
    "        rawdata + metadata_dict -> .fif + .json\n",
    "\n",
    "        :param newfilepath:\n",
    "        :param dataset_metadata:\n",
    "        :param replace:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # create a new information structure\n",
    "        rawdata = fif_raw.get_data(return_times=False)\n",
    "        assert rawdata.shape[0] == dataset_metadata['number_chans']\n",
    "\n",
    "        fif_raw.save(datafilepath,\n",
    "                     overwrite=replace,\n",
    "                     verbose='ERROR')\n",
    "\n",
    "        # create a filepath for the json object\n",
    "        dataset_metadata['filename'] = os.path.basename(datafilepath)\n",
    "        newmetafilepath = datafilepath.replace('_raw.fif', '.json')\n",
    "\n",
    "        # save the formatted metadata json object\n",
    "        writejsonfile(dataset_metadata, newmetafilepath, overwrite=replace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = 'clevelandtvb'\n",
    "patid = 'nl22'\n",
    "patid = 'tvb11'\n",
    "modality = 'seeg'\n",
    "patdir = os.path.join(f\"/Users/adam2392/Downloads/tngpipeline/{center}/{patid}/{modality}/edf/\")\n",
    "patdir = os.path.join(f\"/home/adam2392/hdd/data/rawdata/{center}/{patid}/{modality}/edf/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/adam2392/hdd/data/rawdata/clevelandtvb/tvb11/seeg/edf/split/TVB11_SEEG_SZ_11P_-_12P.edf', '/home/adam2392/hdd/data/rawdata/clevelandtvb/tvb11/seeg/edf/split/TVB11_SEEG_SZ_9P_-_10P.edf', '/home/adam2392/hdd/data/rawdata/clevelandtvb/tvb11/seeg/edf/split/TVB11_SEEG_SZ_8P_-_NCS.edf', '/home/adam2392/hdd/data/rawdata/clevelandtvb/tvb11/seeg/edf/split/TVB11_SEEG_SZ_14P_-_15P_-_16P.edf']\n"
     ]
    }
   ],
   "source": [
    "orig_dataset_ids = {\n",
    "    \"sz_8p_-_ncs\": [\"8p\", \"ncs\"],\n",
    "    \"sz_9p_-_10p\": [\"9p\", \"10p\"],\n",
    "    \"sz_11p_-_12p\": [\"11p\", \"12p\"],\n",
    "    \"sz_14p_-_15p_-_16p\": [\"14p\", \"15p\", \"16p\"]\n",
    "}\n",
    "datadir = os.path.join(patdir, f\"split/\")\n",
    "edffiles = [os.path.join(datadir,f) for f in os.listdir(datadir) if f.endswith('.edf')]\n",
    "\n",
    "print(edffiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split EDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['+0.000000', '+121.810000', '+128.752000', '+128.903000', '+133.614000', '+144.769000', '+334.464000', '+343.380000', '+344.255000', '+357.795000', '+86.177000', 'A1+A2 OFF', 'END', 'SPK run F', 'SZ 11P (good video)', 'SZ 12P', 'Segment: REC START Tech-Bi EE', 'close eyes', 'end', 'moves pelvice', 'right face tonic']\n"
     ]
    }
   ],
   "source": [
    "for fpath in edffiles:\n",
    "    edffilename = os.path.basename(fpath).lower()\n",
    "    edf_datasetid = \"sz\" + edffilename.split(\"_sz\")[1]\n",
    "    \n",
    "    datasetids = orig_dataset_ids[os.path.splitext(edf_datasetid)[0]]\n",
    "    \n",
    "#     f = pyedflib.EdfReader(fpath)\n",
    "#     print(f.readAnnotations())\n",
    "# #     pyedflib.close_file(f)\n",
    "    \n",
    "#     f._close()\n",
    "    splitter = SplitEDF(fpath, datasetids)\n",
    "    edfconverter = splitter.load_file()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_end(startindex, eventtimes, eventnames, idlist):\n",
    "#     print(startindex)\n",
    "    # go from index to end of the event times\n",
    "    for i in range(startindex, eventtimes.shape[0]):\n",
    "        eventid = eventtimes[i,2]\n",
    "        \n",
    "        # get eventidind\n",
    "        eventidind = np.where(idlist == eventid)[0][0]\n",
    "        eventname = eventnames[eventidind]\n",
    "        \n",
    "        print(eventname)\n",
    "        if \"end\" in eventname:\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sz_11p_(good_video)\n",
      "+128.752000\n",
      "close_eyes\n",
      "+128.903000\n",
      "moves_pelvice\n",
      "+133.614000\n",
      "right_face_tonic\n",
      "+144.769000\n",
      "end\n",
      "[121760      0     15]\n",
      "15 sz_11p_(good_video)\n",
      "6 14\n",
      "[144640      0     19]\n",
      "15 sz_11p_(good_video)\n"
     ]
    }
   ],
   "source": [
    "# print(datasetids)\n",
    "\n",
    "for datasetid in datasetids:\n",
    "    datasetid = \"sz_\" + datasetid\n",
    "    \n",
    "    # get np.array of eventtimes\n",
    "    eventtime_arr = edfconverter.event_times\n",
    "    \n",
    "    # get eventids and preprocess the names\n",
    "    event_ids = edfconverter.event_ids\n",
    "    eventnames = list(event_ids.keys())\n",
    "    idlist = [event_ids[name] for name in eventnames]\n",
    "    eventnames = [\"_\".join(name.lower().split(\" \")) for name in eventnames]\n",
    "    \n",
    "    # get index for dataset start\n",
    "    dataset_startind = [ind for ind, name in enumerate(eventnames) if datasetid in name][0]\n",
    "    \n",
    "    # get the id\n",
    "    startid = idlist[dataset_startind]\n",
    "    startname = eventnames[dataset_startind]\n",
    "    \n",
    "    # get the index for id\n",
    "    startind = np.where(eventtime_arr[:,2] == startid)[0][0]\n",
    "\n",
    "    # find the end of this dataset\n",
    "    endind = find_end(startind, eventtime_arr, eventnames, idlist)\n",
    "    \n",
    "    print(eventtime_arr[startind, :])\n",
    "#     print(datasetstart)\n",
    "    print(startid, startname)\n",
    "    \n",
    "    print(startind, endind)\n",
    "    print(eventtime_arr[endind, :])\n",
    "    print(idlist[endind], eventnames[endind])\n",
    "    \n",
    "#     pprint(event_ids)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sz_11p_(good_video)\n",
      "spk_run_f\n"
     ]
    }
   ],
   "source": [
    "print(eventnames[dataset_startind])\n",
    "print(eventnames[endind-1])\n",
    "# print(eventtime_arr)\n",
    "# print(idlist)\n",
    "# pprint(eventnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['+0.000000', 'A1+A2 OFF', 'Segment: REC START Tech-Bi EE']\n",
      "{'+0.000000': 1, 'A1+A2 OFF': 2, 'Segment: REC START Tech-Bi EE': 3}\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "event_times, event_ids = mne.events_from_annotations(edfconverter.raw, \n",
    "                                                     verbose=True,\n",
    "#                                                      regexp=None,\n",
    "                                                    event_id=None)\n",
    "\n",
    "print(event_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 0 3]\n",
      " [0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "print(event_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adam2392/hdd/data/rawdata/clevelandtvb/tvb11/seeg/edf/split/TVB11_SEEG_SZ_11P_-_12P.edf\n",
      "[[0 0 1]\n",
      " [0 0 3]\n",
      " [0 0 2]] {'+0.000000': 1, 'A1+A2 OFF': 2, 'Segment: REC START Tech-Bi EE': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(edfconverter.filepath)\n",
    "print(edfconverter.event_times, edfconverter.event_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resave EDF File and Load In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids = [\"sz_4p\", \"sz_5p\"]\n",
    "\n",
    "for dataset_id in dataset_ids:\n",
    "    fname = os.path.join(patdir, f\"{patid}_{dataset_id}.edf\")\n",
    "    events_list = combined_metadata['events']\n",
    "\n",
    "    write_edf(combined_rawfif, fname, events_list, picks=None, \n",
    "              tmin=0, tmax=None, overwrite=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawedf = mne.io.read_raw_edf(fname, preload=True)\n",
    "\n",
    "print(rawedf)\n",
    "\n",
    "rawdata = rawedf.get_data()\n",
    "print(rawdata.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,20))\n",
    "for i in range(len(combined_rawfif.ch_names)):\n",
    "    ax.plot(np.r_[i]+rawdata[i,20000:20400] / max(rawdata[i,20000:20400]))\n",
    "    \n",
    "    break\n",
    "#     if i == 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Combined FIF Into Another EDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eztrack",
   "language": "python",
   "name": "eztrack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
